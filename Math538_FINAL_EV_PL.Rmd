---
title: " "
output:
  pdf_document:
    number_sections: TRUE
geometry: "left = 2cm, right = 2cm, top = 2cm, bottom = 2cm"
fontsize: 12pt
header-includes:
  - \usepackage{float}
  - \usepackage{sectsty}
  - \usepackage{paralist}
  - \usepackage{setspace}\setstretch{1.25}
  - \usepackage{fancyhdr}
  - \usepackage{lastpage}
  - \usepackage{dcolumn}
  - \usepackage{natbib}\bibliographystyle{agsm}
  - \usepackage[nottoc, numbib]{tocbibind}

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(tinytex.verbose = TRUE)
library(tidyverse)
library(ISLR2)
library(MASS)
library(class)
library(e1071)
library(ggplot2)
library(tidyquant)
library(mice)
library(brms)
library(dplyr)
library(bayesplot)
library(VIM)

```

\allsectionsfont{\centering}
\subsectionfont{\raggedright}
\subsubsectionfont{\raggedright}

\pagenumbering{gobble}

\begin{centering}

\vspace{3cm}


\vspace{1cm}

\vspace{1cm}

\Large

\doublespacing
{\bf Bayesian Missing Data - Multiple Imputation Methods }

\vspace{1 cm}

\normalsize
\singlespacing

\normalsize
Presented to


\includegraphics[width=0.30\textwidth, height=0.30\textheight, keepaspectratio]{images/NSM-STACKED-color.png}




California State University, Fullerton

Math 538 Fall 2023

Dr. Poynor






\vspace{1.5 cm}

\vspace{1.5 cm}

\normalsize

Prepared by

Paul LOPEZ


Emilio VASQUEZ



\vspace{1.5 cm}


\vspace{1.5 cm}

\normalsize
December 07, 2023

\end{centering}

\newpage

\tableofcontents

\newpage


# Introduction

Missing data is an unavoidable issue that arises in many real-world datasets across various fields, including finance, healthcare, and social sciences. When data are missing, it can introduce biases and lead to invalid statistical inferences if the missing values are simply ignored or eliminated. Multiple imputation is a principled approach for handling missing data that involves creating multiple complete versions of the incomplete dataset, with the missing values imputed through simulated draws from an appropriate model. The key insight is that missing data uncertainty can be represented by generating multiple imputed datasets.

In the Bayesian approach to multiple imputation, prior distributions are specified for the model parameters and missing data, representing initial beliefs before examining the data. The posterior distributions of the parameters and missing values are estimated by fitting the model to the observed data using Markov chain Monte Carlo (MCMC) methods. This allows simulating multiple imputed datasets by drawing missing values from their posterior predictive distributions. The completed datasets can then be analyzed using standard complete-data methods, with final estimates pooled across the imputed datasets to incorporate missing data uncertainty. Bayesian multiple imputation provides a flexible framework for handling missing data while properly representing imputation uncertainty.

A key advantage of the Bayesian approach to multiple imputation is that it allows incorporating appropriate prior information and modeling complex relationships between variables. For example, hierarchical priors can be used to share information between related parameters or models, improving estimates for variables with limited data. Bayesian models like mixtures and nonparametric models provide flexibility to adapt to complex patterns in the data. MCMC provides a convenient computational approach for fitting Bayesian models with missing data, avoiding analytical intractability. The posterior predictive distribution for the missing values conditions on both the observed data and model parameters, ensuring proper imputation uncertainty. Practical implementations utilize packages like `mice` in R and `miceforest` in Python, which automate iterative Bayesian modeling, imputation, and analysis. Overall, Bayesian multiple imputation provides a robust approach for handling missing data that accounts for uncertainty and allows incorporating flexible modeling and prior information about the data structure.

\newpage

# Motivation

## Missing Data


Unlike working with data in a classroom setting, real-life data is messy and almost never "clean" out of the box. Often times one can have records that have non-uniform inputs captured. Maybe someone left in a free-form field for entering in a birthday so all the birthdays entered in by the users may all be in different formats. The biggest dilemma that many data professionals will face in their data science careers is the missing data. Missing data come in the following flavors:

### Missing Completely At Random (MCAR)

Missing data can come in the form of missing completely at random. This is when the missing data is completely random. For example, imagine students are taking a survey regarding study habits. At random, some of these students spilled liquid on their survey accidentally making these unable or some portion unusable. In this scenario, missing of the data is completely at random as there is not a systematic pattern related to the missing data. This scenario can be accounted for. 

### Missing at Random (MAR)

Missing at random is when the rate of missing data can be explained if one knew of some other factor. Missing data is common in financial datasets especially due to non-response on surveys or forms. For example, customers may not fill out all the fields on a loan application or investors may skip questions on an investment risk tolerance questionnaire. However, this missing data might be able to be explained by some other question that they answered on the survey, perhaps maybe on occupation. So in theory if one observed that customers who happened to be teachers skipped questions at a higher rate than those who are not teachers, this missing data could be solved if by knowing the occupation.


### Missing Not At random (MNAR)

Missing not at random is when the missing data is related to the variable of interest. For example, suppose that one is collecting data on income, age, and home ownership status in which some individuals refuse to disclose their income. You notice that individuals who refuse to disclose their income happen to make more than those who disclose. Missing data of this nature cannot be necessarily accounted for. 

For the types of missing data above, it becomes imperative for a data scientist or statistician to have tools to overcome the obstacle of missing data. Depending on the scenario such as having a small dataset, throwing out incomplete records could result in losing valuable information and bring potential biases into any estimates provided to a stakeholder. Multiple imputation is one way to predict missing values based on patterns in the observed data. This provides a principled way to fill in missing values while accounting for uncertainty, rather than ad-hoc methods like mean imputation. Some models that can be used are multivariate normal imputation or chained equations/sequential regression approaches. The multiple imputed datasets can then be used to train the predictive models, with results appropriately combined across the imputed datasets which allow building more robust models on messy real-world data.


# Structure of Model/Process


The model structure is a standard Bayesian regression model relating a continuous response Y to predictor variables X:

$$Y \sim N(\mu, \sigma)$$
$$\mu = \beta_0 + \beta_1 X_ 1 + \beta_2 X_2 + ...$$

where:

* Y is the response variable with some values missing

* $X_1, X_2$, etc are the predictor variables

* $\beta_0$ is the intercept

* $\beta_1, \beta_2$, etc are the regression coefficients

* $\sigma$ is the error standard deviation

In addition, we have missingness indicator variables R for each observation, where R = 1 if Y is observed and R = 0 if Y is missing for that observation.

The priors on the parameters $\beta_0, \beta_1, \beta_2, \text{ and } \sigma$ are weakly informative normals or half-normals:

$$\beta_0 \sim N(0, a_1)$$

$$\beta_1 \sim N(0, a_2)$$

$$\beta_2 \sim N(0, a_3)$$

$$\sigma \sim HalfNormal(0, c)$$

Where $a_1, a_2, a_3,...,c$ are constants.  For the missing Y values, we treat them as parameters to be estimated, with priors based on the observed data likelihood:

$$Y_{miss} \sim N(\mu, \sigma)$$

where $\mu$ and $\sigma$ come from fitting the model on the observed data. This makes the missing values exchangeable with the observed data.

Multiple imputation is done by drawing missing Y values from their posterior predictive distribution based on the fitted model. Multiple datasets are created by repeating this process and used to account for missing data uncertainty.

In summary, the model leverages Bayesian regression, weakly informative priors, and treats missing values as parameters to plausibly fill in gaps while quantifying uncertainty. The multiple imputed datasets integrate over the posterior distribution of the missing data.

# Example


The model being fit in this analysis is a Bayesian linear regression model with time-series data. The formula for the model is:

$$close_t = \beta_0 + \beta_1 close_{t-1} + \epsilon_t$$

* $close_t$ is the dependent variable, representing the close of the AAPL stock on day t. 

* $close_{t-1}$ is the independent variable, which is the close of the AAPL stock on the previous day t-1.

* $\beta_0$ is the intercept of the regression line, which represents the expected value of $r_t$, when $r_{t-1}$ is zero.

* $\beta_1$ is the slope coefficient, indicating how much $r_t$ is expected to increase when $r_{t-1}$ increases by one unit.

* $\epsilon_t$ is the error term, which accounts for the variability in $return_t$ that is not explained by $r_{t-1}$.

The Bayesian aspect of this model comes from the use of priors and the Bayesian inference process. Instead of just finding point estimates for $\beta_0$ and $\beta_1$ as in classical regression, the Bayesian approach estimates the entire posterior distributions for these parameters based on the prior distributions and the observed data.

The following sections will delve into the steps taken for this analysis.

## Data Acquisition

The `tq_get` function from the `tidyquant` package retrieves historical stock price data for Apple Inc (APPL) from an online source.

```{r data-read}
set.seed(555)

# Get Apple stock data
stocks <- tq_get("AAPL", get = "stock.prices", from = "2013-01-01")

# Help make sure we don't accidentally add new data
stocks <- stocks[stocks$date <= as.Date('2023-11-17'),]

# Store pre-amp
stocks$close_preamp <- stocks$close
```

```{r, helper-function, echo = FALSE, warning=FALSE, message=FALSE}
################################################################################
# Helper functions
################################################################################

ts_line <- function(df, xval, yval, title ='',  ylab = '', col_in = 'black'){
  # This functions takes in a dataframe along with a x
  p <- ggplot(df, aes_string(x = xval, y = yval), color = col_in) +
    geom_line() +
    labs(title = title, x = "Date", y = ylab) + 
    theme_bw() +
    scale_y_continuous(labels = scales::dollar_format(scale = 1, prefix = "$"))
  return(p)
}

hist_gg <- function(data, x_var, title = "Histogram", x_axis_label = "X Axis", binwidth = NULL) {
  plt <- ggplot(data = data, aes(x = {{x_var}})) +
    geom_histogram(binwidth = binwidth) +
    ggtitle(title) +
    xlab(x_axis_label) + 
    theme_bw()
  return(plt)
}


density_gg <- function(data, x_var, title = "Histogram", x_axis_label = "X Axis") {
  plt <- ggplot(data = data, aes(x = {{x_var}})) +
    geom_density() +
    ggtitle(title) +
    xlab(x_axis_label) + 
    theme_bw()
  return(plt)
}
```


## Exploratory Data Analysis

### Examining APPL

```{r, appl_ts_complete_data, echo = FALSE, warning=FALSE, message=FALSE}

ts_line(stocks, "date", "close", title = 'Apple Stock since January 2, 2013', ylab = 'Price(Close)') +
  # 7:1 split
  geom_vline(aes(xintercept = as.numeric(as.Date('2014-06-09')), color = "Split 7:1"), linetype = "dashed") +
  # 4:1 split
  geom_vline(aes(xintercept = as.numeric(as.Date('2020-08-31')), color = "Split 4:1"), linetype = "dashed") +
  # Adding a legend at the bottom with no title
  scale_color_manual(values = c("Split 7:1" = "red", "Split 4:1" = "blue"), labels = c("7:1 Split", "4:1 Split")) +
  theme(legend.position = "bottom", legend.title = element_blank())
```
$$\textbf{Figure X.}\text{ Historical trend of APPL}  $$

With the help of Tidyquant, historical results can be provided for APPL since January 2, 2013. Overall there has been an upward trend in the daily high stock price. Within this time frame, the stock price started off at \$19.60 and closed at \$189.69 on November 17, 2023 which is a 162.54\% increase in only a decade of data. 

During this time frame, the stock split twice. The first occurred on June 9, 2014 where the stock split 7:1. So the number of shares was multiplied by 7 and saw a share price divided by 7. Following this split, there was not much of a jump in price in the time following this split. The second stock 4:1 stock split on August 31, 2020 saw an upward surge in price in the weeks leading up to and after the stock split.


### Raw Returns Analysis


```{r, raw return analysis}
# Calculate raw returns
returns =diff(log(stocks$close))

# Plot time series  
plot(returns, type="l", main="Raw Apple Returns Since January 2, 2013")
```

$$\textbf{Figure X.}\text{ Historical Trend of Raw Returns for APPL since January 1, 2013}  $$
In the above figure, one can see the historical trend of raw returns for APPL since January 1, 2013. Taking the log of the returns is a common application in fincance used for volatility analysis. Applying the 'diff' function calcualtes the difference between consecutive element. Within the context of this analysis, the function will compute the daily log returns by calculating the difference between each day's log closing price and the previous day closing's prices.

```{r, raw returns ACF plot}
# Calculate autocorrelations
acf_returns = acf(returns, lag.max=5, plot=FALSE) 

# Plot ACF
plot(acf_returns, main="ACF Plot of Raw Returns")
```

$$\textbf{Figure X.}\text{ ACF Plot of raw returns}  $$


With the ACF plots shown in the above figure, the autocorrelation structure of the raw returns. With the autocorrelation falling within the blue horizontal bands, this would mean that the there is independence within the log returns.

##  Data Amputation Prep and Procedure

A subset of the retrieved data is selected, focusing on the 'close' (closing price) and 'volume' (number of shares traded) columns.

```{r}
# Select multiple columns for the amputation process
stocks_for_ampute <- stocks[, c("close", "volume")]
```

The `set.seed` function sets the random number generator's seed to ensure reproducibility of the results. The mice::ampute function artificially introduces missing values into the dataset to simulate incomplete data, which is common in real-world scenarios. The few key arguments are used as follows:

* The prop argument specifies the proportion of data to be made missing

* The mech argument specifies the missingness mechanism as 'MAR' (Missing At Random)

```{r, data amputation}
# Introduce missing values
amputed_data <- mice::ampute(stocks_for_ampute, prop = 0.3, mech = "MAR")

# Update the stocks with the amputed data for 'close'
stocks$close <- amputed_data$amp[, "close"]

# Add flag for complete data. 
# 1 - complete data, 0 - incomplete data
stocks$r <- ifelse(is.na(stocks$close), 0, 1)

# Ensure all necessary columns are present after amputation
stocks <- as.data.frame(stocks)
```

##  Data Preparation Post-Amputation

The stocks dataframe is updated with the amputated 'close' prices.

```{r, amputated_EDA, echo = FALSE, message= FALSE}
ts_line(stocks, "date", "close", title = 'Apple Stock since January 2, 2013 - (30% Missing Data)', ylab = 'Price (Close)')
```

$$\textbf{Figure X.}\text{ Visualization of the amputated data} $$
In the above figure, one can visually see how the mice package amputated the data. Although it could be hard to see due to the visualization, it appears there is a sizable gap in data a few months prior to the year 2021. Again, this was done at random.


## Data Imputation via Predictive Mean Matching

The `mice` function is used to impute missing values in the dataset. It uses Predictive Mean Matching ('pmm') method to fill in missing 'close' prices. Multiple imputed datasets are generated (specified by m = 10) to account for the uncertainty of the imputation process.

```{r, data_imputation_chunk}
# Impute missing values using Predictive Mean Matching

#Assuming your dataframe is named 'your_data'
stock_subset <- stocks[, c("symbol", "date", "close")]

#stock_subset <- stocks

imputed <- mice(data = stock_subset,
                m = 10,
                maxit = 30,
                method = 'pmm',
                seed = 555,
                printFlag = FALSE)

stocks_comp <- complete(imputed)

# View complete and incomplete records
#aggr(stocks_comp)

stocks$close <- stocks_comp$close

# Create lag variable based on imputed data
stocks$lagged_close <- c(NA, head(stocks$close, -1))

View(stocks)

# Extract imputed datasets
imp_datasets <- lapply(1:10, function(i) complete(imputed, i))

#View(imp_datasets)
```

The PMM Process for this mice (Multivariate Imputation by Chained Equations) function works as follows:

i) Fitting of the initial model: We have our variables generically as X (predictors) and Y(variable with missing values).  We fit a regression model:

$$\hat{Y} = f(X;\theta), \text{ where }\theta\text{ are the estimated parameters}$$

ii) For a missing $Y_i$, we find the set of cases(called donors in the package) $D_i$ where $\hat{Y_d} \in D_i$ are closest to $\hat{Y_i}$.

iii) Randomly select a $Y_d$ from $D_i$ to impute $Y_i$. There is the option to do a straight imputation or choose n $Y_d$ and take an average. Straight imputation was performed in this analysis and the selection from this set D gives each candidate an equal chance of being selected. Upon exploring the mice documentation, this probability process was akin to a uniform probability distribution over the pool of potential donors. 

The PMM method is a standalone, non-Bayesian imputation method. The imputed datasets generated are then used further using Bayesian methods in this project.  By creating multiple imputations (m = 10), `mice` acknowledges and represents the uncertainty inherent in the imputation process.


### Analyzing Imputated Data

```{r}
ts_line(stocks, "date", "close_preamp", title = 'Data after PMM imputation method is run to fill in missing values', col_in = "pink") + geom_line(aes(x = date, y= close), color = "red", linetype = 'dotted')
```
$$\textbf{Figure X.}\text{ Historical trend of APPL}  $$

```{r}
stocks$diff <- stocks$close - stocks$close_preamp

hist_gg(stocks, diff, title = "Histogram of Difference of Imputed and Original Data", x_axis_label = "Difference")
```
$$\textbf{Figure X.}\text{ Historical trend of difference between imputed data and original data}  $$


## Bayesian Regression Model

### Setting Priors

Priors are defined for the Bayesian regression model using the `prior` function. The priors reflect our beliefs about the parameters before seeing the data. In this case, normal priors are set for the intercept and slope (return and lagged_return coefficients).

```{r, defining_priors}
# Define the priors for the Bayesian regression model
priors <- c(
  prior(normal(0, 2), class = "Intercept"), # Intercept Prior
  prior(normal(0, 1), class = "b") # Slope Prior
)
```

* Intercept Prior: A normal distribution with a mean of 0 and a standard deviation of 2. This is specified by prior(normal(0, 2), class = "Intercept"). A prior such as this indicates that before looking at the data, an analyst might believe the average daily return when the lagged return is zero is likely to be around 0 but would like to reflect some uncertainty, which is captured by the standard deviation of 2.

* Slope Prior: A normal distribution with a mean of 0 and a standard deviation of 1. This is specified by prior(normal(0, 1), class = "b"). It suggests that before analyzing the data, an analyst might expect the impact of the previous day's return on today's return to be small, as indicated by the mean of 0. The standard deviation of 1 reflects uncertainty about this expectation.


### Fitting a Bayesian Regression Model

The `brm` function from the `brms` package fits a Bayesian regression model to each imputed dataset. The models predict 'return' based on 'lagged_return'. The `iter`, `warmup`, and `chains` arguments control the Markov Chain Monte Carlo (MCMC) sampling process used to estimate the posterior distribution of the model parameters.

The posterior distribution combines the prior distribution with the likelihood of the observed data to update our beliefs about the model parameters. After fitting the model using `brm`, which internally uses Hamiltonian Monte Carlo (HMC) sampling, a type of MCMC, one can obtain a distribution for each parameter that reflects all the information from the priors and the data.

```{r, bayesian_regression_model_fit}
# Initialize an empty list to store the models
fits <- vector("list", length = 5)

# Fit a Bayesian regression model to each imputed dataset and check for errors
for (i in seq_along(imp_datasets)) {
  dataset <- imp_datasets[[i]]
  fit <- tryCatch(
    {
      model <- brm(close ~ lagged_close, data = dataset,
                   iter = 750, warmup = 500, chains = 2, prior = priors)
      model # Return the model
    },
    error = function(e) {
      cat(sprintf("Error in fitting model %d: %s\n", i, e$message))
      NULL # Return NULL if there was an error
    }
  )
  fits[[i]] <- fit
}

```


```{r, posterior_intercept}

# Extract posterior samples for the first fitted model
post_samples <- posterior_samples(fits[[1]], pars = c("Intercept", "lagged_close", "sigma"))

# Histogram for the Intercept
hist(post_samples$b_Intercept, breaks = 30, main = "Posterior of Intercept", 
     xlab = "Intercept", col = "blue")

density_gg(post_samples, b_Intercept, "Posterior of Intercept", expression(beta ~ 'Intercept'))

```


```{r, posterior_lagged_return, echo = FALSE, message = FALSE, warning = FALSE}

hist(post_samples$b_lagged_close, breaks = 30, main = "Posterior of lagged_close", 
     xlab = "lagged_close", col = "green")

density_gg(post_samples, b_lagged_close, "Posterior of Lagged Return", 'Lagged Close')
```

```{r, sigma_post, echo = FALSE, warning = FALSE, message= FALSE}
# Histogram for sigma
hist(post_samples$sigma, breaks = 30, main = "Posterior of Sigma", 
     xlab = "Sigma", col = "red")

density_gg(post_samples, sigma, expression('Posterior of' ~ sigma), expression(sigma ~ 'Posterior'))
```


```{r, table_of_estimates}
# Initialize lists to store parameter estimates and standard errors
param_estimates <- list()
param_se <- list()

# Extract parameter estimates and standard errors from each model
for (i in seq_along(fits)) {
  if (!is.null(fits[[i]])) {
    # Extracting estimates
    est <- as.data.frame(summary(fits[[i]])$fixed)[, "Estimate"]
    param_estimates[[i]] <- est
    
    # Extracting standard errors
    se <- as.data.frame(summary(fits[[i]])$fixed)[, "Est.Error"]
    param_se[[i]] <- se
  }
}

# Calculate the mean of the parameter estimates
combined_estimates <- Reduce("+", param_estimates) / length(fits)

# Calculate the pooled standard error if necessary
combined_se <- sqrt(Reduce("+", lapply(param_se, `^`, 2))) / length(fits)

# Combine into a data frame
combined_results <- data.frame(Estimate = combined_estimates, Std.Error = combined_se)

# Print combined results
#print(combined_results)

# Initialize lists to store CIs
ci_lower <- list() 
ci_upper <- list()

for (i in seq_along(fits)) {
  
  if (!is.null(fits[[i]])) {
    
    # Extract CIs 
    ci <- as.data.frame(summary(fits[[i]])$fixed)[, c("l-95% CI", "u-95% CI")]
    
    ci_lower[[i]] <- ci[,1]
    ci_upper[[i]] <- ci[,2]
    
  }
  
}

# Combine CIs 

ci_lower_combined <- Reduce("+", ci_lower) / length(fits)
ci_upper_combined <- Reduce("+", ci_upper) / length(fits)

combined_results$ci_lower <- ci_lower_combined
combined_results$ci_upper <- ci_upper_combined

print(combined_results)
```



## Comparing with OLS

```{r, ols_model}
# OLS model
fit_ols <- lm(stocks$close ~ stocks$lagged_close)
summary(fit_ols)

```

## Combining of Model Results

* Parameter estimates and standard errors are extracted from each fitted model.  These estimates are combined by calculating the mean estimate and pooled standard error for each parameter across all imputed datasets.


## Summary and Analysis

The combined results are printed out for interpretation. 

This analysis allows for Bayesian inference on time-series data with missing values. The choice of prior values should be justified based on domain knowledge or previous research. Since normal priors are used for both the intercept and slope, it implies a belief that the coefficients are likely to be around 0 but allowing for some variation.

The mice::ampute function is crucial because it enables researchers to understand how well their imputation and analysis methods work when some data is missing, which is a common occurrence in real-world datasets.

The brm function is central to the Bayesian approach, as it fits a model within the Bayesian framework using Hamiltonian Monte Carlo, a type of MCMC sampling. The function returns a wealth of information about the model, including estimates of the posterior distribution of the model parameters, which reflects both the data and the priors.


# Conclusion

This analysis demonstrated a Bayesian approach for handling missing data, specifically using multiple imputation to fill gaps while quantifying uncertainty. The mice R package enabled missing data simulation on the AAPL stock dataset. Five imputed complete datasets were then generated using predictive mean matching. Bayesian regression models were fitted to predict daily returns based on prior day returns, combining prior beliefs about the coefficients with likelihood from the observed data to estimate posterior distributions. Hamilton Monte Carlo sampling enabled convenient fitting of the posteriors. Parameter estimates were finally pooled across the multiple imputations to obtain overall inference, incorporating missing data uncertainty.

The multi-step process provides a blueprint for principled missing data handling - imputing while preserving relationships in the data, fitting appropriate Bayesian models, and aggregating estimates for final inference. The methodology can generalize across applications with missing observations, from financial time series forecasting to political polls, epidemiology, econometrics, or climate data analysis. Missingness often arises when merging multiple datasets as well. The multiple imputation framework allows proceeding with modeling and prevents relying only on complete cases. The Bayesian approach allows flexible incorporation of complex relationships and prior knowledge to strengthen imputations and parameter estimates despite gaps. Use of MCMC sampling handles high-dimensional posteriors despite analytic intractability. Thus Bayesian missing data techniques serve as broadly applicable tools for overcoming an omnipresent analysis challenge to enable robust modeling from incomplete real-world data.

# Reference

1. Lai, Mark. 2019. "Course Handouts for Bayesian Data Analysis Class." Class handouts, December 13, 2019. https://bookdown.org/marklhc/notes_bookdown/missing-data.html

2. Little, Roderick. 2011. "Calibrated Bayes, for Statistics in General, and Missing Data in Particular." Statistical Science 26 (2): 162–74. https://doi.org/10.1214/10-STS318.

